"use strict";

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.getChatParams = void 0;
var _constants = require("@kbn/stack-connectors-plugin/common/openai/constants");
var _uuid = require("uuid");
var _constants2 = require("@kbn/stack-connectors-plugin/common/bedrock/constants");
var _server = require("@kbn/langchain/server");
var _constants3 = require("@kbn/stack-connectors-plugin/common/gemini/constants");
var _prompt = require("../../common/prompt");
/*
 * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
 * or more contributor license agreements. Licensed under the Elastic License
 * 2.0; you may not use this file except in compliance with the Elastic License
 * 2.0.
 */

const getChatParams = async ({
  connectorId,
  model,
  prompt,
  citations
}, {
  actions,
  request,
  logger
}) => {
  const abortController = new AbortController();
  const abortSignal = abortController.signal;
  const actionsClient = await actions.getActionsClientWithRequest(request);
  const connector = await actionsClient.get({
    id: connectorId
  });
  let chatModel;
  let chatPrompt;
  let questionRewritePrompt;
  let llmType;
  switch (connector.actionTypeId) {
    case _constants.OPENAI_CONNECTOR_ID:
      chatModel = new _server.ActionsClientChatOpenAI({
        actionsClient,
        logger,
        connectorId,
        model,
        traceId: (0, _uuid.v4)(),
        signal: abortSignal,
        temperature: (0, _server.getDefaultArguments)().temperature,
        // prevents the agent from retrying on failure
        // failure could be due to bad connector, we should deliver that result to the client asap
        maxRetries: 0
      });
      chatPrompt = (0, _prompt.Prompt)(prompt, {
        citations,
        context: true,
        type: 'openai'
      });
      questionRewritePrompt = (0, _prompt.QuestionRewritePrompt)({
        type: 'openai'
      });
      break;
    case _constants2.BEDROCK_CONNECTOR_ID:
      llmType = 'bedrock';
      chatModel = new _server.ActionsClientSimpleChatModel({
        actionsClient,
        logger,
        connectorId,
        model,
        llmType,
        temperature: (0, _server.getDefaultArguments)(llmType).temperature,
        streaming: true
      });
      chatPrompt = (0, _prompt.Prompt)(prompt, {
        citations,
        context: true,
        type: 'anthropic'
      });
      questionRewritePrompt = (0, _prompt.QuestionRewritePrompt)({
        type: 'anthropic'
      });
      break;
    case _constants3.GEMINI_CONNECTOR_ID:
      llmType = 'gemini';
      chatModel = new _server.ActionsClientSimpleChatModel({
        actionsClient,
        logger,
        connectorId,
        model,
        llmType,
        temperature: (0, _server.getDefaultArguments)(llmType).temperature,
        streaming: true
      });
      chatPrompt = (0, _prompt.Prompt)(prompt, {
        citations,
        context: true,
        type: 'gemini'
      });
      questionRewritePrompt = (0, _prompt.QuestionRewritePrompt)({
        type: 'gemini'
      });
      break;
    default:
      break;
  }
  if (!chatModel || !chatPrompt || !questionRewritePrompt) {
    throw new Error('Invalid connector id');
  }
  return {
    chatModel,
    chatPrompt,
    questionRewritePrompt,
    connector
  };
};
exports.getChatParams = getChatParams;